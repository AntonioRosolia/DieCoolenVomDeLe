{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Approximation Theorem - Gradient Descent Optimisation\n",
    "\n",
    "Here we study the possibility to approximate functions with a MLP with a single hidden layer (n1 hidden units).\n",
    "As activation functions, we use the sigmoid ('logit') function.\n",
    "\n",
    "We generate training data - by assuming a function on the unit interval [0,1]. Here, we provide two families of functions:\n",
    "* Beta distribution function: $b_{\\alpha,\\beta}(x)=x^\\alpha\\cdot(1-x)^\\beta$\n",
    "* Sine function: $sin_\\omega(x)=\\sin(2\\pi\\omega\\cdot x)$\n",
    "\n",
    "Finally, we use mini-batch-gradient descent to minimize MSE cost.\n",
    "\n",
    "Goals:\n",
    "* Learn how a given function can be represented with a single layer MLP;\n",
    "* Understand that, in principle, it can be learned from sample data;\n",
    "* Understand that the optimization based on plain gradient is not always straightforward; \n",
    "* Experience that the choice of the hyper-parameters number of hidden units, batchsize, learning rate is tricky. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_function(x,y):\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('x')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_compare_function(x,y1,y2, label1='', label2=''):\n",
    "    plt.plot(x, y1, label=label1)\n",
    "    plt.xlabel('x')\n",
    "    plt.plot(x, y2, label=label2)\n",
    "    if label1 and label2:\n",
    "        plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1. / (1. + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,W1,b1,W2,b2):\n",
    "    \"\"\"\n",
    "    Computes the output for the single hidden layer network (n1 units) with 1d input and 1d output.\n",
    "    \n",
    "    Arguments:\n",
    "    W1 -- weights of the hidden layer with shape (n1,1)\n",
    "    b1 -- biases of the hidden units with shape (n1,1)\n",
    "    W2 -- weights of the output layer with shape (1,n1)\n",
    "    b2 -- bias of the output\n",
    "    X  -- input data with m samples and shape (1,m)    Only ONE Dimension for Feature Points\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- Output from the network of shape (1,m) \n",
    "    \n",
    "    Berechne die Prediction von einem Multi Layer Perceptron\n",
    "    1 Layer: ypred = sigmoid(W*X + b)\n",
    "    2 Layer: ypred = (W2*(W1* X + b1)+ b2)     (no sigmoid function!!!!)\n",
    "    \n",
    "    DImensionen Bsp:  [1,3]*([3,1]*[1,5] + [3,1]) + 1\n",
    "                      [1,3]*[3,5] + 1\n",
    "                      [1,5] = One Dimensional Result for 5 inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START YOUR CODE ###\n",
    "    m = X.shape[1]\n",
    "    n = W1.shape[0]\n",
    "\n",
    "    A1 = sigmoid(np.dot(W1,X) + b1)\n",
    "    A2 = np.dot(W2, A1) + b2\n",
    "    \n",
    "    '''\n",
    "    h1 = np.dot(W1,X.T) + b1\n",
    "    z1 = sigmoid(h1)\n",
    "    h2 = np.dot(W2,z1) + b2\n",
    "    # z2 = sigmoid(h2)   NO SIGMOID FUNCTION on last level \n",
    "    A2 = h2\n",
    "    #A2 = np.exp(h2)/(np.sum(np.exp(h2)))    no idea why no softmax\n",
    "    '''\n",
    "   \n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99805844 1.04946333 1.09991675 1.14913132 1.19690185]]\n"
     ]
    }
   ],
   "source": [
    "W1 = np.array([0.4,0.2,-0.4]).reshape(3,1) # n1 = 3\n",
    "b1 = np.array([0.1,0.1,0.1]).reshape(3,1)\n",
    "W2 = np.array([1,2,1]).reshape(1,3)\n",
    "b2 = -1\n",
    "X = np.linspace(-1,1,5).reshape((1,5))\n",
    "Ypred = predict(X,W1,b1,W2,b2)\n",
    "print(Ypred)\n",
    "Yexp = np.array([0.99805844, 1.04946333, 1.09991675, 1.14913132, 1.19690185]).reshape(1,5)\n",
    "np.testing.assert_array_almost_equal(Ypred,Yexp,decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(Y,Ypred):\n",
    "    \"\"\"\n",
    "    Computes the MSE cost for given ground truth Y and predicted Ypred\n",
    "    Uses the predict function defined above.\n",
    "    \n",
    "    Arguments:\n",
    "    Y -- ground truth output with shape (1,m) \n",
    "    Ypred -- predicted output with shape (1,m) \n",
    "    \n",
    "    Returns:\n",
    "    cost -- the MSE cost divided by 2.\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    cost = 1/(np.size(Ypred)*2) * np.sum((Y - Ypred)**2)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invSigmoid(z):\n",
    "    return(sigmoid(z) * (1 - sigmoid(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST - Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.10347685 2.99140124 6.38635147 7.83999104 7.98562875]]\n",
      "9.016690991741989\n"
     ]
    }
   ],
   "source": [
    "W1 = np.array([4,5,6]).reshape(3,1)\n",
    "W2 = np.array([1,2,3]).reshape(1,3)\n",
    "b1 = np.array([1,1,1]).reshape(3,1)\n",
    "b2 = 2\n",
    "X = np.linspace(-1,1,5).reshape(1,5)\n",
    "Ypred = predict(X,W1,b1,W2,b2)\n",
    "print(Ypred)\n",
    "Y = 2.0*np.ones(5).reshape(1,5)\n",
    "c = cost(Y,Ypred)\n",
    "print(c)\n",
    "cexp = 9.01669099\n",
    "np.testing.assert_almost_equal(c,cexp,decimal=8)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(W1,b1,W2,b2,X,Y):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the MSE cost for a single hidden layer network with 1d input and 1d output.\n",
    "    The parts of the gradient associated with the weights array and bias array for the hidden layer, \n",
    "    the weights array and the bias for the output layer are provided as separate numpy arrays of according \n",
    "    dimension. \n",
    "    \n",
    "    Arguments:    \n",
    "    W1 -- weights of hidden layer with shape (n1,1)\n",
    "    b1  -- biases of hidden layer with shape (n1,1)\n",
    "    W2 -- weights of output layer with shape (1,n1)\n",
    "    b2  -- biases of output layer\n",
    "    X  -- input data with shape (1,m)\n",
    "    Y  -- labels with shape (1,m)\n",
    "    \n",
    "    m = nr of samples\n",
    "    n1 = nr of hidden layer Points\n",
    "    \n",
    "    ACHTUNG: NUR 1 dimensionaler Input X und Ooutput ypred\n",
    "    \n",
    "    To calculate the gradient we calculate the derivatives of all the Signals no their parapeters\n",
    "    And then with the chain rule we can multiply there derivatives\n",
    "    \n",
    "    aE/aW1 = aE/ah2  * ah2/ah2in  ah2in/aW1 * \n",
    "    \n",
    "    Returns:\n",
    "    gradient -- dictionary with the gradients w.r.t. W1, W2, b1, b2 and according keys \n",
    "                'dW1' with shape (n1,1)\n",
    "                'db1' with shape (n1,1)\n",
    "                'dW2' with shape (1,n1)\n",
    "                'db2' a scalar\n",
    "                \n",
    "             \n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "\n",
    "    m = X.shape[0]\n",
    "    n1 = W1.shape[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #V1\n",
    "    #dw with mse is: 1/m * sum(ypred * (1 - ypred) *( ypred - y)* X)   (generally)\n",
    "    #for neurons: yred = A1, but the \"cost comparison\" still has to be done by ypred\n",
    "    #dW1 = (1/m*np.sum(diff*A1.T*(1-A1)*X, axis=1)*W2).T \n",
    "    #dW2 = (1/m*np.sum(diff*A1, axis=1)).reshape(1,n1)\n",
    "    #db1 = (1/m*np.sum(diff*A1*(1-A1), axis=1)*W2).T\n",
    "    #db2 = 1/m*np.sum(diff)\n",
    "    '''\n",
    "    V2\n",
    "    dW1 = (1/m*np.sum(diff*A1*(1-A1)*X, axis=1)*W2).T\n",
    "    dW2 = (1/m*np.sum(diff*A1, axis=1)).reshape(1,-1)\n",
    "    db1 = (1/m*np.sum(diff*A1*(1-A1), axis=1)*W2).T\n",
    "    db2 = 1/m*np.sum(diff)\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    v3\n",
    "    dW1 = 1/(m) * np.sum(np.dot(np.dot(np.dot(np.dot(diff , A1.T),(1 - A1)), X),  W2).T)\n",
    "    dW2 = 1/(m) * np.sum(np.dot(diff,A1.T))    \n",
    "    db1 = 1/(m) * np.sum( np.dot(np.dot(np.dot(diff,A1.T),(1 - A1)).T,W2))\n",
    "    db2 = 1/m*np.sum(diff)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #V4\n",
    "    if(m == 1):\n",
    "        print(X)\n",
    "    \n",
    "    \n",
    "    X = X.reshape(1,m)\n",
    "    print(\"X : \",X.shape)\n",
    "    z = np.dot(W1, X) + b1              # calculate input of 1st neuron\n",
    "    AL1 = sigmoid(z)                      #calculate Activation of 1st neuron\n",
    "    \n",
    "    AL2 = predict(X, W1, b1, W2, b2)      #calculate Activation of last (2nd) neuron\n",
    "    yPred = AL2\n",
    " \n",
    "    print(\"Ypred : \", yPred.shape)\n",
    "    print(\" Y : \", Y.shape)\n",
    "    diff = yPred - Y.T                      #calc error (for siplicity reasons)\n",
    "    \n",
    "    #apply chain rule derivatives for all dWs and dBs\n",
    "    \n",
    "    #dW2 change of cost(error) under change of W2 (on Level L2)\n",
    "    dW2 = 2 * np.dot(diff, invSigmoid(AL1).T)        #sigmoid(AL1) * (1 - sigmoid(AL1)) = derivative of Sigmoid relative to AL1\n",
    "    db2 = 2 *np.dot(diff, invSigmoid(1).T)           #input of sigmoid = 1 (because derivative of AzL to Abl = 1)\n",
    "    \n",
    "    #on 1st Level (L1) of neurons (propagate further back)\n",
    "    print(diff.shape)\n",
    "    print(np.dot(invSigmoid(W2).T,diff).shape)\n",
    "    dW1 = 2 * np.dot(np.dot(invSigmoid(W2).T,diff ), invSigmoid(X).T)\n",
    "    db1 = 2 * np.dot(np.dot(invSigmoid(W2).T,diff ), invSigmoid(1).T)\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    ### END YOUR CODE ###\n",
    "    return {'dW1':dW1, 'dW2':dW2, 'db1':db1, 'db2':db2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST - Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :  (7, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (3,1) and (3,7) not aligned: 1 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-1b69856966d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mgradJ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mdW1exp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.00590214\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.00427602\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.00234663\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-133-d146112b456a>\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(W1, b1, W2, b2, X, Y)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X : \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m)\u001b[0m              \u001b[1;31m# calculate input of 1st neuron\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mAL1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m                      \u001b[1;31m#calculate Activation of 1st neuron\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (3,1) and (3,7) not aligned: 1 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "#n1 = 3  (3 neurons)\n",
    "#m = 7   (7 samples)\n",
    "\n",
    "W1 = np.array([4,5,6]).reshape(3,1)\n",
    "W2 = np.array([1,2,3]).reshape(1,3)\n",
    "b1 = np.array([1,1,1]).reshape(3,1)\n",
    "b2 = 2\n",
    "X = np.array([1,2,3,4,5,6,7]).reshape((7,1))\n",
    "Y = np.array([2,2,2,2,2,2,2]).reshape((1,7))\n",
    "gradJ = gradient(W1,b1,W2,b2,X,Y)\n",
    "\n",
    "dW1exp = np.array([0.00590214,0.00427602,0.00234663]).reshape(W1.shape)\n",
    "db1exp = np.array([0.00579241,0.004247,0.00234079]).reshape(b1.shape)\n",
    "dW2exp = np.array([5.99209251,5.99579451,5.99714226]).reshape(W2.shape)\n",
    "db2exp = 5.99792323\n",
    "print(gradJ['dW1'])\n",
    "#print(gradJ['db1'])\n",
    "    \n",
    "#print(gradJ['dW2'])\n",
    "#print(gradJ['db2'])\n",
    "np.testing.assert_array_almost_equal(gradJ['dW1'],dW1exp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(gradJ['db1'],db1exp,decimal=8)\n",
    "np.testing.assert_array_almost_equal(gradJ['dW2'],dW2exp,decimal=8)\n",
    "np.testing.assert_almost_equal(gradJ['db2'],db2exp,decimal=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,Y,n1,nepochs,batchsize=32,learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Performs the training by using MBGD for a MLP with a single hidden layer (n1 units) and 1d input and output layer.\n",
    "    Input:\n",
    "    n1 = nr of neurons in hidden layer\n",
    "    \n",
    "    \n",
    "    It starts with initializing the parameters:\n",
    "    * the weights and the biases for the hidden units : W1,b1 of shape (n1,1) \n",
    "    * the weights and the bias for the output layer: W2 of shape (1,n1) and scalar b2 \n",
    "\n",
    "    Then, it loops over the epochs and per epoch over the mini-batches. The number of batches is determined from the \n",
    "    batchsize.\n",
    "    \"\"\"\n",
    "    # initialize weights and bias in Dimensions for n1 neurons in hidden layer\n",
    "    W1 = np.random.uniform(-1,1,n1).reshape(n1,1)*0.05\n",
    "    b1 = np.zeros((n1,1),dtype='float')\n",
    "    W2 = np.random.uniform(-1,1,n1).reshape(1,n1)*0.05\n",
    "    b2 = 0.0\n",
    "    \n",
    "        \n",
    "    #m = X.shape[1]\n",
    "    m = np.size(X)\n",
    "    mb = int(m/batchsize)      #nr of mini batches\n",
    "    indices = np.arange(m)     #indices of one mini Batch (from 1st to last item)\n",
    "    np.random.shuffle(indices) # shuffle indices \n",
    "    # remember the epoch id and cost after each epoch for constructing the learning curve at the end\n",
    "    costs = [] \n",
    "    epochs = []\n",
    "\n",
    "    # Initial cost value:\n",
    "    epochs.append(0)\n",
    "    Ypred = predict(X.T,W1,b1,W2,b2)\n",
    "   \n",
    "    costs.append(cost(Y,Ypred)) \n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(nepochs):\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        #shuffle \n",
    "        np.random.shuffle(indices)\n",
    "        deltaW = []\n",
    "        #iterate with MiniBatch\n",
    "        \n",
    "        for iteration in range(mb):\n",
    "            print(iteration)\n",
    "            #splitting of Set into mini batches\n",
    "            #xMini = X[:,iteration*batchsize:(iteration+1)*batchsize]\n",
    "            #yMini = Y[:,iteration*batchsize:(iteration+1)*batchsize]\n",
    "            \n",
    "            #miniBatches witch miniBatch Class\n",
    "            batches = MiniBatches(X, Y, batchsize)\n",
    "            \n",
    "            #calculate gradient\n",
    "            gradJ = gradient(W1, b1, W2, b2, batches.getX(), batches.getY())\n",
    "            #adjust params\n",
    "            W1 = W1 - learning_rate*gradJ['dW1']\n",
    "            b1 = b1 - learning_rate*gradJ['db1']\n",
    "            W2 = W2 - learning_rate*gradJ['dW2']\n",
    "            b2 = b2 - learning_rate*gradJ['db2']\n",
    "            deltaW.append(gradJ['dW1'])\n",
    "            batches = batches.next()\n",
    "        #print('epoch: {}, cost: {}'.format(epochs[-1], costs[-1]))\n",
    "        #print(deltaW)\n",
    "        ### END YOUR CODE ###\n",
    "        epochs.append(epoch+1)\n",
    "        Ypred = predict(X,W1,b1,W2,b2)\n",
    "        costs.append(cost(Y,Ypred))         \n",
    "    \n",
    "    #print(costs[-1])    \n",
    "    params = {'W1':W1, 'W2':W2,'b1':b1,'b2':b2}    \n",
    "    return params, np.array(epochs), np.array(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatches():\n",
    "    \n",
    "    def __init__(self, x, y, batchsize):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        m = x.shape[1]\n",
    "        if not batchsize:\n",
    "            self.batchsize = m\n",
    "        else:\n",
    "            self.batchsize = batchsize\n",
    "        self.n = x.shape[0]\n",
    "        self.mb = int(m/batchsize)\n",
    "        self.indices = np.arange(m)\n",
    "        np.random.shuffle(self.indices)\n",
    "        self.ib = 0\n",
    "        \n",
    "    def getX(self):\n",
    "        return self.x\n",
    "    \n",
    "    def getY(self):\n",
    "        return self.y\n",
    "    \n",
    "    def number_of_batches(self):\n",
    "        return self.mb\n",
    "        \n",
    "    def next(self):\n",
    "        it = self.indices[self.ib*batchsize:(self.ib+1)*batchsize]\n",
    "        xbatch = self.x[:,it].reshape(self.n,batchsize)\n",
    "        ybatch = self.y[:,it].reshape(1,batchsize)\n",
    "        self.ib += 1\n",
    "        return xbatch, ybatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_fct(x,alpha,beta):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    x - input array\n",
    "    alpha, beta -- larger values lead to more pronounced peaks\n",
    "    \"\"\"\n",
    "    c = alpha/(alpha+beta)\n",
    "    norm = c**alpha*(1-c)**beta\n",
    "    return x**alpha*(1-x)**beta/norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sin_fct(x,omega):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    x -- input array\n",
    "    omega -- frequency that defines the integer number of cycles within the unit interval\n",
    "    \"\"\"\n",
    "    return np.sin(x*2*np.pi*omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputs(m, func, random=True, vargs=None):\n",
    "    \"\"\"\n",
    "    Generates m (x,y=f(x))-samples by either generating random x-values in the unit interval (random=True) or by \n",
    "    generating a grid of such values. Then the y values (used as labels below) are created from the function object \n",
    "    `func`.\n",
    "    Parameter needed to define the function `func` can be passed as vargs-dict. \n",
    "    \"\"\"\n",
    "    if random:\n",
    "        x = np.random.rand(1,m)\n",
    "        y = func(x, **vargs)\n",
    "    else:\n",
    "        x = np.linspace(0,1,m).reshape(1,m)\n",
    "        y = func(x,**vargs)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000\n",
    "func = beta_fct\n",
    "vargs={'alpha':2.0,'beta':2.0}\n",
    "#func = sin_fct\n",
    "#vargs={'omega':3.0}\n",
    "\n",
    "X,Y = generate_inputs(m,func,vargs=vargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1bfdbd1eb00>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAePElEQVR4nO3df2yc910H8Pen7mKvwq3rxpOYk86O5pOIFqSupzqRJygcRm6M2n8KbtE0xqo1FG1EBiFdFbSQomjHEBgDZbhC1dikrS79AyIcUpmj1ZjlmDjsR2imnoNtiJ2JmrrOLHV2WufDH+fHPd99n+eeOz/3/Hy/pKq+5576vk99/vh7n+fz/XxFVUFERNF3R9ADICIibzCgExHFBAM6EVFMMKATEcUEAzoRUUzcGdQL79+/X7u6uoJ6eSKiSLp8+fL/qWqH6bnAAnpXVxdmZ2eDenkiokgSkf+2e44pFyKimGBAJyKKCQZ0IqKYYEAnIooJBnQiopioWuUiIi8C+BUAb6nqJwzPC4BRAMcBvAvgs6r6H14PlKjRhsamcfXGTRz+6D24euMmrpwZQF8ujx//5D1cOTMQ9PCIqnJTtvg1AH8F4Os2zz8CoGf7n14AX93+N1FojUwWcHH+bRw9dB9e/M487v7wh7C8tgEAmFlY3TnPOgYAR05fwOc+dQivXL6OA/fehfETx3wfN5GTqgFdVb8tIl0OpzwG4Ota7MN7UUTaROSnVfVHHo2RyBPWDBwA1je3AHwQvK3HTtY3tzCanwNQDPSpU+dxa6vYfvpkpgfD/alGDJvINS9y6J0Arpc8Xto+VkFEnhaRWRGZXVlZ8eCliZylTp3H0Ng0gGLwXt/cchW8AaArO2H82mIFcwAYzc/hyOkLexwt0d54EdDFcMy4a4aqvqCqaVVNd3QYV64SeWJksoC+XB63tnRXCqUWi7nBms5f39zC0Ng0UqfOY2SyUNdrEu2FFwF9CcDBkscHANzw4PsS1WxksoDUqfMYzc/tyn+bZti1cBvcZxZWcWtLMZqfQ18uv6fXJKqVFwH9HIDPSNFRADeZP6cgDI1NYzQ/tysVUove7na0Njft/BsAOttadr6u1fLaBo6cvsDZOvnGTdnitwA8DGC/iCwBOA3gQwCgqn8D4DyKJYvXUCxb/M1GDZbIZGhsGpcWV3G7hjh+MtNTUeViqlqZymZ2vm5tbtpV5eImlWPdSL04/zarYqjhJKhNotPptLLbIu1VXy6/K7ViZzE3uKvKxYu68tIqFzdam5tw94c/tOuPBFGtROSyqqaNzzGgU1QdOX3BVcVKb3d7w2fH1gIkN+O5Q4D5L9d2w5XI4hTQufSfIunQsxNVg2dnWws621p8SXVMZTO4cmYAvd3t2NdkKvz6wG0FSxypIThDp0g59OxE1Vz5vibBMw9/PNCFPlaFi1M6iDN1qofTDD2wHYuIajUyWXAM5r3d7Vh6591Q5KitMTilhW5rsZyys60lFGOm6GNAp0gYGpt2rCq5QxDKKpIrZwZ2+sbYjX95bQMjkwW2DqA9Y0Cn0Ku2KMiPm557YQVqp9m61SOGQZ32gjdFKdScVlu2NjdhMTcY6mBe6sqZAZzM9Ng+z9WltFcM6BRaqVPnHW8qRrFH+XB/Cou5QdtKmOW1jT23KaDkYkCnUHJatGPNzKOscPa4Y0sBq0MkUS0Y0Cl0hsambYP5Ym4wkjNzkytnBmyD+szCKnvAUM0Y0ClUUqfOG6tB9jUJOttaAhhRYznl1Ufzc0y/UE0Y0Ck0jpy+YJyZ72sSFM4ej22t9nB/yvGPFW+UklsM6BQKI5OFnZK+0vx4b3c7CmePBzUs30xlM7ZB3apTJ6qGS/8pcHaLhqyZeZKMTBZ2atLLcUUpAWzORSFmF8xbm5sSF8yBYvrF7kaptWEGkR0GdArMyGTBGMxPZnpiU8lSD6drd7vBNSUTAzoF5quvXzMevzj/ts8jCZ/F3KBt9UtXdoI5dTJiQCffjUwW0JWdMFa0+NW/PAqG+1O4w6a1+mh+jouPqAKbc5HvXrl83Xg86qs/G2H+y4O2PeCPHrrP/wFRqHGGTr6y2wO02i4/SfbFX7RfeMSbpFSKAZ18MzJZMAbzpFa0uOXU0Gt9c4v5dNrBgE6+6MvlbeurD3/0Hp9HE00P3H+v8fiL35n3eSQUVgzo5IsD995lPH4y08OboC6NnzhmrFFf39xi6oUAMKCTT0z15ou5Qe7QU6MrZwbQ291ecXx9c4tVL8SATo1llSiaMADVx666ZWZhlf9PE44BnRrGqS9J2PcBDbPh/pTtoiP2UU82BnRqmL/8V3Mwb21uYjDfI6egblfnT/HHgE4N0ZfLGxfD7GuSRPdp8dJwf8qYT2e73eRiQCfP2dWbd7a1sN7cY3b59NH8HIN6AjGgk6eGxqZt8+Z2pYtUP7tZOsB+L0nEgE6+4E3Qxhk/ccx2t6Old971eTQUJFcBXUQGRORNEbkmIlnD8/eLyGsi8l0R+YGI8HN1Ajn1N2cwb6zHHzxoPM5NMZKlakAXkSYAzwN4BMBhAE+KyOGy0/4AwMuq+gCAJwD8tdcDpXBzSrVQ4zntdMRFR8nhZob+EIBrqjqvqrcAvATgsbJzFMDd21/fA+CGd0OkKLC7OXcy08PVoD65cmbA2IKY6a7kcBPQOwGUFrYubR8r9YcAPi0iSwDOA/ii6RuJyNMiMisisysrK3UMl8LKFLS5tD8Y5TdJZxZWuctRQrgJ6KZG1eUVxk8C+JqqHgBwHMA3RKTie6vqC6qaVtV0R0dH7aOl0LGW9puW9zOABMNq4mVX/ULx5SagLwEoveNyAJUplacAvAwAqjoNoAXAfi8GSOFm17q1tbmJs/MAHf7oPRU3qFnGGH9uAvolAD0i0i0i+1C86Xmu7Jz/AZABABH5GRQDOnMqCXD3hz9kPP65Tx3yeSRUig28kqlqQFfV9wF8AcCrAH6IYjXLGyLynIg8un3a7wH4vIh8H8C3AHxWVQ0LvylOhsamjStCe7vbOTsPmLXLkanfCxt4xZerTaJV9TyKNztLj32p5OurAPq8HRqFnbVoZTE3uJND50bP4fLV16/ZHucf3fjhSlGq2dDYNLqyEzuzcyuY261WpOA88/DHjcdvbSln6THkaoZO5AZ7tYTPcH9qZyZeWonEtFg8cYZONenL5Y3L+zvbWrh4JcTKb4Ratem8QRovDOjkml1bXMC+lwiFg12TLjbvihcGdNozLu8Pv6lsxnYzDM7S44MBnVwzLSLi8v7oGD9xjGWMMceATlVZVS3rm1sVzzEQRMvF+bdrOk7RwoBOVbGTYnxYfV7KzSyssm96DLBskRwdOX3BODNnr5bosjbpLi1jXMwN8tNWDDCgk6MrZwYqOilyZh595cG79GfMn210MeVCtqzWuBQ/Vq8XK/1itWxgLj3aGNDJaGSyYNxSjrPzeLG6Ylp/uLkZRrQxoJMRqyGSwal5F0UPAzpVGJksGJf3U/wUzh43Lji6taVccBRBDOhUwW52tq9J2K+FKMQY0GmXkckCbm1V7k1yMtODwtnjAYyIGm38xDFj6+OZhVX05fIBjIjqxYBOu7xy+XpNx4koPBjQaUdfLm/sptjZ1oKpbCaAEZFfprIZY5+X5bUNVrxECAM6AbDfHxTgxhVJYdWmm45TNDCgk6Pe7nbeCE0Q02ycdenRwYBOjmWK3AAhWezWGYzm5xjUI4ABnWx/iXu725k7Txi7nukUDQzoZEypLOYGmWqhXUbzc1xsFHIM6AlnbV5hOk7JNNyfsp2l2/XGp3BgQE8wu9z5yUwPZ+cJN9yfMrYE4Cw93BjQE4wNuIjihQE9oTg7p2rs0iszC6ucpYcUAzoRGdmlXSi8GNATaGhs2rh5RW93O1cF0i52ZYwzC6usSw8hBvQEsvsozQoGMjFVvXS2tfCPfwi5CugiMiAib4rINRHJ2pzzayJyVUTeEJFvejtM8gpn51SP8m6by2sb6MpOsL1uyNxZ7QQRaQLwPIB+AEsALonIOVW9WnJOD4BnAfSp6jsi8pFGDZjq57TEn7NzcrKyvlnTcQqGmxn6QwCuqeq8qt4C8BKAx8rO+TyA51X1HQBQ1be8HSY1Ejd+pmoKZ48bN8G4taWcpYeIm4DeCaD089bS9rFSKQApEZkSkYsiMmD6RiLytIjMisjsyspKfSOmuoxMFoypFiK37Noos71yeLgJ6GI4Vr5H2Z0AegA8DOBJAH8rIm0V/5HqC6qaVtV0R0dHrWOlBuDsnNxixUv4uQnoSwAOljw+AOCG4Zx/VNX3VHUBwJsoBngKCVPQXswNMphTTey2ImR73XBwE9AvAegRkW4R2QfgCQDnys75BwC/AAAish/FFMy8lwMlb7FFKtVjKpsx5tIB7jsbBlUDuqq+D+ALAF4F8EMAL6vqGyLynIg8un3aqwDeFpGrAF4D8PuqyoYgIWB1UyzvqDian+PNLKrL4w8erOk4+UdUy9Ph/kin0zo7OxvIayeJ3c1Q5s5pL4bGpo0lsNyysPFE5LKqpk3PVa1Dp2gb7k/hlcvXd20AbdoImKgWVtAu/+THYB4sLv2PMSvdUhrMreNEe3Xk9IWKY13ZCeNx8gcDeoyVz5ZOZnq4tRw13PrmFu/PBIQBPab6cnneCKWGunJmwLZaijdIg8GAHkMjk4WKNIuFv2hE8cWbojFkVa+UVrfwRig1gum9VvqYlVT+YtlizLCcjIJSmuJrbW7ClTPGlk60R05li0y5xAw3r6AglFdOrW9uoSs7wYoqnzHlEiNcRERBsT79lb8H+anQXwzoMXJx3txtwe44kZeOnL6A9c2tXce6shNMv/iIKZeYYO6cgva5Tx0yHl/f3GLqxSecocfE0UP3GQM6c+fkl+H+1E5qr/QGKSus/MOAHgNOuxEx3UJ+K1/6bwV3pl4ajymXGDuZ6WG6hXxnl3qxO07e4Qw94vpyeeOq0M62Fla2UCDsUi98PzYeZ+gRx80GKIxGJgsVvYS6shPcpq7BOEOPMKfKFs6GKEhsCRAMztAjjKtCiagUZ+gRxdw5RRVn6Y3DgB5RU9kMANb7UnjZpV2ocRjQI8iu7nxkssBZD4UKc+n+Yg6diBpquD9VsbMRb9w3BgM6ETWU6RPlzMIqurIT3BLRY0y5RIypox1QXFbNGQ+F0XB/Chfn3zaW2B64964ARhRfnKFHjKkXxmJukD0yKNTGTxxDb3d7xfGZhVV2YvQQA3qEmFbfWceJwm78xLGKXHpnWwv7DXmIKZeIYN05xUF598/ltQ10ZSfYt98jnKFHhF2ukTlIipLxE8ewmBvclX5pbW5iMPcIZ+gR4HQjlL8IFDXlnzatDaU721p2FsxRfURVA3nhdDqts7Ozgbx2FJU34uKqUIq60vc0NzJ3T0Quq2ra9Bxn6BFRGsxN1QJEUVL+qXM0P4fR/Bx3NdojVzl0ERkQkTdF5JqIZB3Oe1xEVESMfz2odkNj0xWVLSz1oqjjrkaNUTWgi0gTgOcBPALgMIAnReSw4bxWAL8DYMbrQSaVU79z5s4pykztAIDiTJ1luPVzM0N/CMA1VZ1X1VsAXgLwmOG8PwLwFQCVtXVUl6s3btZ0nChK7DYw58bm9XMT0DsBXC95vLR9bIeIPADgoKr+k9M3EpGnRWRWRGZXVlZqHmyS9OXyxsqWzrYW5hgpFsZPHENnW0vF8ZmFVfZ4qZObgC6GYzulMSJyB4ARAL9X7Rup6guqmlbVdEdHh/tRJtDK+mZNx4mI3AT0JQClOw4fAHCj5HErgE8AeF1EFgEcBXCON0brNzQ2jVtbleWkvd3tKJw9HsCIiBpjKpsxVm0tr23wxn8d3AT0SwB6RKRbRPYBeALAOetJVb2pqvtVtUtVuwBcBPCoqrLIvE7MnRNRPaoGdFV9H8AXALwK4IcAXlbVN0TkORF5tNEDTJqRyYIxd34y08PcOcUSc+necbWwSFXPAzhfduxLNuc+vPdhEVGSmPbI5erR2nGlaIg41Z3zjU1xVz4bt1aPsseLe+y2GCJHD91X03EiolIM6CExNDZdse8iwNk5JcdUNmNcPbq8tsHVoy4xoIcEZ+dEtFfMoYcAdyMiKrLe7+WfVq3H/H1wxhl6CPz4J+/VdJyIyIQz9IA51Z1zNkJJxFl6/RjQA2Z683I3Iko6dmKsDwN6gOzqzofGptnvnBLNev+PTBZ2JjutzU0sEqiCOfQAsbKFyN6R0xd2fXJd39zCaH4OR05fCHBU4caAHpDSmUcp5s6Jiq6cGcBibrCiGyO3qbMnqpVtWv2QTqd1djbZDRnLUy7MnRPtVr6ZdKmkTn5E5LKqGtuTc4YegJHJArqyExX5c66GI9rtypkBY790gDdITRjQA1A+qziZ6cFibjCRsw2ies0srHISVIYB3WdDY9O7WoQCxZJF7s5CZDZ+4pixxwtVYkD30chkwVimeDLTwzJFIgevXL5e0/GkYkAnotDj3qPuMKD7aLg/tetN2dvdztw5kUt26zOYS/8AA7pPTJUtfCMSuTfcn2IuvQoGdCKKDLtSRRYWFDGg+4C7ERF5w6nihS0z2Jyr4ZwqWxjMiWrH9rr2OENvMNObi8GcaG/KCwxKjycZA3qDld/0ZJqFqHG6shOJzqUzoDeIVdVS/rGQlS1E3rDLpyf5d4w59AbhTkREjee0gjSJn4QZ0BvArtd5V3aC+XMiahimXBrAbtbQ2dbCYE7koalsBp1tLRXHl9c20JfLBzCiYDGge2xksoDltQ3jc48/eNDn0RDFn93v1fLaRuJy6QzoHrNbycbqFqLGsCthBJK3CYargC4iAyLypohcE5Gs4fnfFZGrIvIDEcmLyMe8H2r42S0iAriKjaiRxk8cMwb1mYXVRJUxVg3oItIE4HkAjwA4DOBJETlcdtp3AaRV9WcBvALgK14PNAo4OycKztI779Z0PI7czNAfAnBNVedV9RaAlwA8VnqCqr6mqtb/tYsADng7zPDj5hVEwZrKZox16UnKpbsJ6J0ASss2lraP2XkKwD+bnhCRp0VkVkRmV1ZW3I8y5OzKFInIX8P9KeN6j6R8QnYT0MVwTI0ninwaQBrAn5ieV9UXVDWtqumOjg73oww5likShYdpNt6VnUjELN1NQF8CUFoXdADAjfKTROSXAJwC8KiqbnozvPA7cvqCsUyxtbkJU9lMACMiSrbh/hRam5sqjo/m53Dk9IUARuQfNwH9EoAeEekWkX0AngBwrvQEEXkAwBiKwfwt74cZXpvv367pOBE13uGP3lPT8bioGtBV9X0AXwDwKoAfAnhZVd8QkedE5NHt0/4EwE8B+HsR+Z6InLP5drHzzMMf3/W4s60Fi7lBFM4eD2hERDR+4hgWc4MVN0njXj4sqsZ0eMOl02mdnZ0N5LW94HQjtLe7nZUtRAEbGps2Vp5F/fdTRC6ratr0HFeK1unF78wbj7c2N0X6zUIUF+Mnjhn7vMwsrMa2zwsDeh1GJgtY39wyPve5Tx3yeTREZOfAvXcZj8e1Np0BvQ5cEUoUDVYuPSnb1bEfeh1MS4m5eQVRtHRlJyKfTy/HGXqNhsamjXXnSWoARBQ1dv1c4ta8iwG9BuzXQhRNdhthAPFq3sWA7lJfLm8sU+TyfqJomMpmjCtIl9c2YrOClAHdpR//5L2gh0BEexT3KjQGdBeOnL5gLFNkvxaiaLHb3Wh9cysWuXQG9CqGxqaNwby3ux1XzgwEMCIi2os4727EgO7AaUu5ON1IIUoau54uUV9FyoDuwKnPOVMtRNE13J8y7m4EAI8/eNB4PAoY0G3Y1Zv3drczmBPF2Gh+LrKzdAZ0G1dv3Kw41tnWwnpzophwmqVHtdcLA7pBV3bCeCN0ZT0xGzERJcJwf8p2wdFXX7/m82j2jgG9zKFnJ4zHW5ubuGkFUQxNZTPGqpdbWxq51AsDeolDz07gtmG/j9bmJpYoEsWYXSlj1FIvDOjbjpy+YAzmABjMiRJsND8Xmfp0BnTYLx4CYHvThIjixW6HIyA6i44SH9Dt9h0EiqkWNt4iSg6nroyXFs1xIkwSH9DtgjmX9hMl01Q2gzuk8vhttS+aCItEB/SurP0Ph/XmRMk1/+XBXTN1a0cyu/tsYZHYgO7U/5h5cyIqnamXTv66shOhnaknMqDbLRwCisGceXMiAoAv/qJ5cndbEcoa9cQFdKc0y2JukMGciHY4rSQNY416ogK6019U06ICIqKpbMY2DTuan8OhZydCE9jvDHoAfrHbdQhgmoWInFnxwbSv8G394HjQcST2Ab0vl8ePbm7Y3p1mMCciN4b7U3jl8nVjW20gHEE9timXkckC+nJ5LK+Zg3lnWwuDORHVxEq/mOrUgQ96qQeVghHVYAor0+m0zs7Oev59+3J5/Pgn79mmVyxWXSkRUT2sCaOdzrYWHLj3Ls/XtIjIZVVNm56LxQw9deo8hsamMTJZwPLahmMwv0NYZ05Ee2fXdteyvLaxs0fpyGTBce2LV1wFdBEZEJE3ReSaiGQNzzeLyPj28zMi0uX1QEtZTXKsqpVbW4qZhVXjDYtSvd3tmP8ySxOJyBtODb0sy2sbGM3PYX1zC0Nj0w2tiqka0EWkCcDzAB4BcBjAkyJyuOy0pwC8o6ofBzAC4I+9Hmgpq/+K08edcq3NTVzOT0SeqzZTLzWzsLqrKsZrbmboDwG4pqrzqnoLwEsAHis75zEAf7f99SsAMiJic9vAG6lT5wE4LxQCinksNtoiokYaP3EMJzM9gadz3ZQtdgK4XvJ4CUCv3Tmq+r6I3ARwH4D/Kz1JRJ4G8DQA3H///TUNtLzN7a2t6jdzO9taMJXN1PQ6RET1sFK5F+fftu3iWsqajHpZbedmhm6aaZdHUzfnQFVfUNW0qqY7OjrcjG/H+IljWMwN7lSnlP+7VGtzE05mehjMich31nZ2VnbAjhXPvLyn52aGvgTgYMnjAwBu2JyzJCJ3ArgHgK/d4Hu723H00H0Agl+tRUTJVnq/ri+Xx+MPHsSL35mvWk69V24C+iUAPSLSDWAZwBMAfr3snHMAfgPANIDHAfyrNrDA3fqrZ91d5gIhIgorK1Mw3J/C0Ng0Li2u2nZx3CtXC4tE5DiAPwfQBOBFVT0rIs8BmFXVcyLSAuAbAB5AcWb+hKrOO33PRi0sIiKKM6eFRa56uajqeQDny459qeTrDQC/updBEhHR3sRipSgRETGgExHFBgM6EVFMMKATEcVEYO1zRWQFwH/X+Z/vR9kq1ATgNSdHEq+b1+zex1TVuDIzsIC+FyIya1e2E1e85uRI4nXzmr3BlAsRUUwwoBMRxURUA/oLQQ8gALzm5EjidfOaPRDJHDoREVWK6gydiIjKMKATEcVEqAN62Dan9oOLa/5dEbkqIj8QkbyIfCyIcXqp2jWXnPe4iKiIRL68zc01i8ivbf+s3xCRb/o9Rq+5eG/fLyKvich3t9/fx4MYp5dE5EUReUtE/tPmeRGRv9j+f/IDEfnknl5QVUP5D4qtev8LwCEA+wB8H8DhsnN+G8DfbH/9BIDxoMftwzX/AoC7tr9+JgnXvH1eK4BvA7gIIB30uH34OfcA+C6Ae7cffyTocftwzS8AeGb768MAFoMetwfX/XMAPgngP22ePw7gn1Hc9e0ogJm9vF6YZ+ih3Jy6wapes6q+pqrvbj+8iOIOUlHm5ucMAH8E4CsANvwcXIO4uebPA3heVd8BAFV9y+cxes3NNSuAu7e/vgeVO6NFjqp+G867tz0G4OtadBFAm4j8dL2vF+aAbtqcutPuHFV9H4C1OXVUubnmUk+h+Nc9yqpes4g8AOCgqv6TnwNrIDc/5xSAlIhMichFERnwbXSN4eaa/xDAp0VkCcX9F77oz9ACVevvvCNXG1wExLPNqSPE9fWIyKcBpAH8fENH1HiO1ywidwAYAfBZvwbkAzc/5ztRTLs8jOKnsH8TkU+o6lqDx9Yobq75SQBfU9U/FZFjAL6xfc23Gz+8wHgaw8I8Q69lc2oEtTm1x9xcM0TklwCcAvCoqm76NLZGqXbNrQA+AeB1EVlEMc94LuI3Rt2+t/9RVd9T1QUAb6IY4KPKzTU/BeBlAFDVaQAtKDawijNXv/NuhTmg72xOLSL7ULzpea7sHGtzasCHzal9UPWat9MPYygG86jnVYEq16yqN1V1v6p2qWoXivcNHlXVKG9I6+a9/Q8o3gCHiOxHMQXjuE9vyLm55v8BkAEAEfkZFAP6iq+j9N85AJ/ZrnY5CuCmqv6o7u8W9F3gKneIjwMooHh3/NT2sedQ/IUGij/wvwdwDcC/AzgU9Jh9uOZ/AfC/AL63/c+5oMfc6GsuO/d1RLzKxeXPWQD8GYCrAK6guPF64ONu8DUfBjCFYgXM9wD8ctBj9uCavwXgRwDeQ3E2/hSA3wLwWyU/5+e3/59c2et7m0v/iYhiIswpFyIiqgEDOhFRTDCgExHFBAM6EVFMMKATEcUEAzoRUUwwoBMRxcT/A+pYzqp84G3BAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X[0,:],Y[0,:],'+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the Input and Output\n",
    "\n",
    "It turns out that it is important to normalize the input and the output data here.\n",
    "Remember the mean and stdev computed for the training data so that you can also apply it to the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X, mu=None, stdev=None):\n",
    "    \"\"\"\n",
    "    Normalizes the data by using z-normalization. If mu and stdev are NOT specified, mean and stedev are \n",
    "    computed from the given samples.   \n",
    "    \n",
    "    Returns:\n",
    "    X1 -- normalized data (array of the same shape as input)\n",
    "    mu -- mean\n",
    "    stdev -- standard deviation\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    mu = np.mean(X)\n",
    "    \n",
    "    stdev = np.std(X)\n",
    "   \n",
    "    X1 = (X - mu)/stdev\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return X1,mu,stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_normalize(X1, mu, stdev):\n",
    "    \"\"\"\n",
    "    Invert the normalization. This is needed to bring the predicted values back to the original scale.\n",
    "\n",
    "    Returns:\n",
    "    X -- unnormalized data (array of the same shape as input X1)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "\n",
    "    X = (X1 * stdev) + mu\n",
    "    \n",
    "       \n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n",
      "[-1.34164079 -0.4472136   0.4472136   1.34164079]\n",
      "2.5\n",
      "1.118033988749895\n",
      "[1. 2. 3. 4.]\n"
     ]
    }
   ],
   "source": [
    "# Test Norm (selfmade)\n",
    "X = np.arange(1,5)\n",
    "print(X)\n",
    "X,mu,stdev = normalize(X)\n",
    "print(X)\n",
    "print(mu)\n",
    "print(stdev)\n",
    "mu_true = 2.5\n",
    "stev_true = 1.118034\n",
    "\n",
    "X = inv_normalize(X,mu,stdev)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(1, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Test Input Normalization\n",
    "x, _, _ = normalize(X)\n",
    "y, _, _ = normalize(Y)\n",
    "#plt.plot(x[0,:],y[0,:],'+')\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the Training\n",
    "\n",
    "Includes generating and normalizing the data and training. Test data can be generated as non-random.<br>\n",
    "Make sure that you do the test performance on the right scales (of both x-values and y-values)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "X :  (1, 1000)\n",
      "Ypred :  (1, 1000)\n",
      " Y :  (1000, 1)\n",
      "(1, 1000)\n",
      "(10, 1000)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1000 into shape (1000,32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-152-5fc28bedfe39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnormalizeData\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minv_normalize\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmuX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdevX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-150-8a5d99792770>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X, Y, n1, nepochs, batchsize, learning_rate)\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgradJ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'db2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mdeltaW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradJ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dW1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;31m#print('epoch: {}, cost: {}'.format(epochs[-1], costs[-1]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m#print(deltaW)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-151-6f2a191f28eb>\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mib\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mib\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mxbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mybatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mib\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 1000 into shape (1000,32)"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# settings\n",
    "mtrain = 1000\n",
    "mtest = 1000\n",
    "func = beta_fct                  #Y   (soll fuktion)\n",
    "#print(func)\n",
    "vargs={'alpha':2.0,'beta':2.0}\n",
    "\n",
    "n_hidden = 10 # number of hidden units\n",
    "nepochs = 100 # number of epochs\n",
    "batchsize = 32\n",
    "learning_rate = 0.1\n",
    "\n",
    "### START YOUR CODE ###\n",
    "\n",
    "# generate data (train and test)\n",
    "m = mtrain + mtest\n",
    "testfactor = mtest/m\n",
    "X,Y = generate_inputs(m,func,random=False,vargs=vargs)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X.T, Y.T, test_size=testfactor, random_state=1)\n",
    "\n",
    "# normalize Data\n",
    "normalizeData = True\n",
    "if normalizeData:\n",
    "    x_train, muX_train, stdevX_train = normalize(x_train)\n",
    "    y_train, muY_train, stdevY_train = normalize(y_train)\n",
    " \n",
    "    \n",
    "# train \n",
    "                 \n",
    "params, epochs, costs =train(x_train,y_train,n_hidden,nepochs,batchsize=batchsize,learning_rate=learning_rate)\n",
    "if normalizeData:\n",
    "    x_train = inv_normalize( x_train, muX_train, stdevX_train)\n",
    "    y_train = inv_normalize(y_train, muY_train, stdevY_train)\n",
    "    \n",
    "    \n",
    "\n",
    "### END YOUR CODE ###\n",
    "\n",
    "plt.semilogy(epochs,costs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "Compute the predicted values on the test set and compute the MSE cost.\n",
    "Prepare a (x,y)-plot with the ground truth test values and the predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n",
      "(1, 1000)\n"
     ]
    }
   ],
   "source": [
    "### START YOUR CODE ###\n",
    "#Wurde schon in Train gemacht mit testsplit\n",
    "'''\n",
    "mtest = 1000\n",
    "Xtest,Ytest = generate_inputs(mtest, func, random=False, vargs=vargs)\n",
    "'''\n",
    "\n",
    "if normalizeData:\n",
    "    x_test, muX_test, stdevX_test = normalize(x_test)\n",
    "    y_test, muY_test, stdevY_test = normalize(y_test)\n",
    "         \n",
    "W1 = params['W1']\n",
    "b1 = params['b1']\n",
    "W2 = params['W2']\n",
    "b2 = params['b2']\n",
    "ypred_test = predict(x_test,W1,b1,W2,b2)\n",
    "\n",
    "if normalizeData:\n",
    "    #x_test = inv_normalize(x_test, muX_test, stdevX_test)\n",
    "    y_test = inv_normalize(y_test, muY_test, stdevY_test)\n",
    "    ypred_test = inv_normalize(ypred_test, muY_test, stdevY_test)\n",
    "\n",
    "\n",
    "### END YOUR CODE ###\n",
    "print(x_test.shape)\n",
    "print(ypred_test.shape)\n",
    "#plt.plot(x_test[0,:],y_test[0,:],'b-')\n",
    "#plt.plot(x_test[0,:],Ypred_test[0,:],'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
